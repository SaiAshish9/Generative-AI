# Generative-AI

```
AI can be classified into two categpries:

1. GenAI

Generating new content like text, images, video, audio

for example ChatGPT


2. Non GenAI

We've data and based on that we are making certain decisions:

a. XRAY: A person has disease or not. 
b. Person should be given loan or not based on credit history data.


Evolution of GenAI:

Early days predicting home price

In the following pic:

Area, Bedrooms and Age are the simple features.

```

<img width="634" height="642" alt="Screenshot 2025-08-06 at 3 49 13 PM" src="https://github.com/user-attachments/assets/96df145a-8ede-44b1-9d64-c324e897d30e" />

```
Complex Features:
```

<img width="385" height="401" alt="Screenshot 2025-08-06 at 3 50 36 PM" src="https://github.com/user-attachments/assets/a6ae8b7f-197c-4f77-bd3b-e41b3d48f744" />


```
Hence, we can't simply use features for image detection hence therefore neural networks were invented and that had given birth to deep learning.
```

<img width="1020" height="668" alt="Screenshot 2025-08-06 at 4 02 52 PM" src="https://github.com/user-attachments/assets/fcac4484-d244-43ff-a8b7-08675387d6a4" />


<img width="916" height="318" alt="Screenshot 2025-08-06 at 4 03 32 PM" src="https://github.com/user-attachments/assets/53a18744-3cc8-4731-b910-31bf7c8a0bc4" />

```
After that came RNN
```

<img width="793" height="583" alt="Screenshot 2025-08-06 at 4 04 15 PM" src="https://github.com/user-attachments/assets/028d6948-7c37-4d49-894b-3f1b608a3f9f" />


```
RNN is used for solving problems like language translation where we provide words to the network.
An RNN (Recurrent Neural Network) is a type of neural network particularly well-suited for sequential data. Unlike traditional feedforward networks, 
RNNs have loops that allow information to persist, making them ideal for tasks involving time-series, text, audio, or any data where the order matters.
```

<img width="731" height="550" alt="Screenshot 2025-08-06 at 4 12 39 PM" src="https://github.com/user-attachments/assets/1ff5185b-70e7-495e-aa3e-aa94bc1bd48d" />

```
In Gmail it will try to autocomplete
```

<img width="679" height="548" alt="Screenshot 2025-08-06 at 4 13 13 PM" src="https://github.com/user-attachments/assets/0cf1a3ec-66e5-49f4-9c43-304d45b973d2" />


```
It looks at the content of the email. Network will read the sentence and predict the probability of the next word.
```

<img width="1110" height="230" alt="Screenshot 2025-08-06 at 4 16 01 PM" src="https://github.com/user-attachments/assets/d4bed52c-e45d-4cc8-85f5-e918654c761f" />
<img width="1120" height="312" alt="Screenshot 2025-08-06 at 5 53 49 PM" src="https://github.com/user-attachments/assets/67aa20e1-cfcb-40c7-b9c7-1fa540fcc551" />
<img width="1001" height="398" alt="Screenshot 2025-08-06 at 5 54 26 PM" src="https://github.com/user-attachments/assets/7e877651-fed1-4b34-95e2-fc515719b0ab" />

```
Reaching out has higher probability
```

```
Language Model is an AI model that can predict the next word (or set of worcs) for a given sequence of words
```

<img width="987" height="597" alt="Screenshot 2025-08-06 at 6 03 41 PM" src="https://github.com/user-attachments/assets/45083f3c-4e53-402e-8b5c-65dd4797841d" />


```
This approach of giving words to neural network is called self supervised learning.

We can generate these training pairs from variety of books
```

<img width="940" height="629" alt="Screenshot 2025-08-06 at 6 08 11 PM" src="https://github.com/user-attachments/assets/09c773f7-bd09-4a35-bcc9-71ad554ea591" />

```
When we feed huge amount of data and the neural network is big, we'll get large language model (LLM)
```

<img width="982" height="650" alt="Screenshot 2025-08-06 at 6 15 44 PM" src="https://github.com/user-attachments/assets/50997142-c1b2-4de4-8787-9d095d4275bd" />

```
GPT-4, the model behind chatgpt is an LLM.
```

<img width="801" height="619" alt="Screenshot 2025-08-06 at 6 17 49 PM" src="https://github.com/user-attachments/assets/a202cf7c-dbf6-442b-80e8-88afd64f825a" />

<img width="1162" height="664" alt="Screenshot 2025-08-06 at 6 19 53 PM" src="https://github.com/user-attachments/assets/dc3d4ca6-d051-43eb-8875-9018a77cc568" />

<img width="1030" height="660" alt="Screenshot 2025-08-06 at 6 20 21 PM" src="https://github.com/user-attachments/assets/d4b33b3f-11a3-41ac-83a9-f6b3dcc83ebe" />

<img width="1201" height="655" alt="Screenshot 2025-08-06 at 6 21 13 PM" src="https://github.com/user-attachments/assets/a3aa3516-bc9a-4d1d-8260-28e5c6fac07b" />

<img width="464" height="525" alt="Screenshot 2025-08-06 at 6 38 21 PM" src="https://github.com/user-attachments/assets/a6306ac5-6d66-4eb7-a516-2e29c233e00c" />

```
Google - BERT

BERT (Bidirectional Encoder Representations from Transformers) is a groundbreaking language model developed by Google AI in 2018.
It's designed to understand the context of words in a sentence more deeply than previous models, thanks to its bidirectional nature.

OpenAI - GPT
```

<img width="691" height="364" alt="Screenshot 2025-08-06 at 6 42 08 PM" src="https://github.com/user-attachments/assets/019473ea-b1bb-4df5-ace0-bca3481712ad" />
<img width="787" height="508" alt="Screenshot 2025-08-06 at 6 43 31 PM" src="https://github.com/user-attachments/assets/2635298b-7f32-4349-a53e-4ccebd778f21" />

```
Text Models: BERT (Google) and GPT (OpenAI)
```

```
Image Models: DALL-E and Stable Diffusion
```

<img width="1100" height="650" alt="Screenshot 2025-08-06 at 6 45 58 PM" src="https://github.com/user-attachments/assets/90d9df22-c350-4e40-a0da-3ead8b20a308" />
<img width="1232" height="657" alt="Screenshot 2025-08-06 at 6 46 27 PM" src="https://github.com/user-attachments/assets/a1d2f138-657e-4074-a78b-65263e42a578" />
<img width="912" height="558" alt="Screenshot 2025-08-06 at 6 57 33 PM" src="https://github.com/user-attachments/assets/b752a87b-0242-4331-89e9-edd360d55517" />

<img width="575" height="362" alt="Screenshot 2025-08-06 at 7 32 20 PM" src="https://github.com/user-attachments/assets/6d5e07ee-b308-4a25-9f9e-95a89a5677fb" />

